{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split conllu en 4 (short, medium, long, mix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "462\n",
      "154\n",
      "154\n",
      "154\n",
      "154\n",
      "Le fichier a été divisé en trois fichiers : /home/or-llsh-156-l01/projets/CRISCO/Gascon/train/split_size/1460_Les_Fors_anciens_de_Bearn_grShort.conllu, /home/or-llsh-156-l01/projets/CRISCO/Gascon/train/split_size/1460_Les_Fors_anciens_de_Bearn_grMedium.conllu, /home/or-llsh-156-l01/projets/CRISCO/Gascon/train/split_size/1460_Les_Fors_anciens_de_Bearn_grLong.conllu\n",
      "Un fichier mixte a été généré : /home/or-llsh-156-l01/projets/CRISCO/Gascon/train/split_size/1460_Les_Fors_anciens_de_Bearn_grMix.conllu\n"
     ]
    }
   ],
   "source": [
    "from conllu import parse_incr\n",
    "import random\n",
    "\n",
    "# Fonction pour trier les phrases d'un fichier conllu par taille (nombre de tokens)\n",
    "def trier_phrases_par_taille(phrases):\n",
    "    # Trier les phrases par taille (nombre de tokens)\n",
    "    phrases_triees = sorted(phrases, key=lambda phrase: len(phrase))\n",
    "    return phrases_triees\n",
    "\n",
    "# Fonction pour diviser un fichier conllu en trois fichiers et générer un fichier mixte\n",
    "def diviser_fichier_conllu_avec_mix(fichier_conllu, fichier_sortie1, fichier_sortie2, fichier_sortie3, fichier_sortie_mix):\n",
    "    with open(fichier_conllu, 'r', encoding='utf-8') as f:\n",
    "        # Lire toutes les phrases dans une liste\n",
    "        phrases_mix = list(parse_incr(f))\n",
    "        print(len(phrases_mix))\n",
    "\n",
    "    phrases = trier_phrases_par_taille(phrases_mix)\n",
    "\n",
    "    # Calculer le nombre de phrases par groupe\n",
    "    total_phrases = len(phrases)\n",
    "    taille_groupe = total_phrases // 3  # Arrondi à l'entier inférieur par division entière\n",
    "\n",
    "    # Répartir les phrases en trois groupes\n",
    "    groupe1 = phrases[:taille_groupe]\n",
    "    groupe2 = phrases[taille_groupe:2 * taille_groupe]\n",
    "    groupe3 = phrases[2 * taille_groupe:]\n",
    "\n",
    "    # Créer un groupe mixte avec le même nombre de phrases que les autres groupes\n",
    "    groupe_mix = []\n",
    "    groupe_mix.extend(random.sample(groupe1, taille_groupe // 3))\n",
    "    groupe_mix.extend(random.sample(groupe2, taille_groupe // 3))\n",
    "    groupe_mix.extend(random.sample(groupe3, taille_groupe - len(groupe_mix)))\n",
    "    random.shuffle(groupe_mix)\n",
    "\n",
    "    # Écrire chaque groupe dans son propre fichier\n",
    "    with open(fichier_sortie1, 'w', encoding='utf-8') as f1:\n",
    "        print(len(groupe1))\n",
    "        for phrase in groupe1:\n",
    "            f1.write(phrase.serialize())\n",
    "\n",
    "    with open(fichier_sortie2, 'w', encoding='utf-8') as f2:\n",
    "        print(len(groupe2))\n",
    "        for phrase in groupe2:\n",
    "            f2.write(phrase.serialize())\n",
    "\n",
    "    with open(fichier_sortie3, 'w', encoding='utf-8') as f3:\n",
    "        print(len(groupe3))\n",
    "        for phrase in groupe3:\n",
    "            f3.write(phrase.serialize())\n",
    "\n",
    "    with open(fichier_sortie_mix, 'w', encoding='utf-8') as f_mix:\n",
    "        print(len(groupe_mix))\n",
    "        for phrase in groupe_mix:\n",
    "            f_mix.write(phrase.serialize())\n",
    "\n",
    "    print(f\"Le fichier a été divisé en trois fichiers : {fichier_sortie1}, {fichier_sortie2}, {fichier_sortie3}\")\n",
    "    print(f\"Un fichier mixte a été généré : {fichier_sortie_mix}\")\n",
    "\n",
    "# Exemple d'utilisation\n",
    "fichier_conllu = \"/home/or-llsh-156-l01/projets/CRISCO/Gascon/train/1460_Les_Fors_anciens_de_Bearn_21.08.24_resort_train.conllu\"\n",
    "fichier_sortie1 = \"/home/or-llsh-156-l01/projets/CRISCO/Gascon/train/split_size/1460_Les_Fors_anciens_de_Bearn_grShort.conllu\"\n",
    "fichier_sortie2 = \"/home/or-llsh-156-l01/projets/CRISCO/Gascon/train/split_size/1460_Les_Fors_anciens_de_Bearn_grMedium.conllu\"\n",
    "fichier_sortie3 = \"/home/or-llsh-156-l01/projets/CRISCO/Gascon/train/split_size/1460_Les_Fors_anciens_de_Bearn_grLong.conllu\"\n",
    "fichier_sortie_mix = \"/home/or-llsh-156-l01/projets/CRISCO/Gascon/train/split_size/1460_Les_Fors_anciens_de_Bearn_grMix.conllu\"\n",
    "\n",
    "diviser_fichier_conllu_avec_mix(fichier_conllu, fichier_sortie1, fichier_sortie2, fichier_sortie3, fichier_sortie_mix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m a\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split conllu en 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "634\n",
      "211\n",
      "422\n",
      "633\n",
      "Le fichier a été divisé en trois fichiers : /home/ziane212/Téléchargements/dump_MICLE_1563_Guernsey_30.08.24/last/expe_LLcD/train/size_split_train/1563_Guernsey_train_gr1.conllu, /home/ziane212/Téléchargements/dump_MICLE_1563_Guernsey_30.08.24/last/expe_LLcD/train/size_split_train/1563_Guernsey_train_gr2.conllu, /home/ziane212/Téléchargements/dump_MICLE_1563_Guernsey_30.08.24/last/expe_LLcD/train/size_split_train/1563_Guernsey_train_gr3.conllu\n"
     ]
    }
   ],
   "source": [
    "from conllu import parse_incr\n",
    "\n",
    "# Fonction pour trier les phrases d'un fichier conllu par taille (nombre de tokens)\n",
    "def trier_phrases_par_taille(phrases):\n",
    "    # Trier les phrases par taille (nombre de tokens)\n",
    "    phrases_triees = sorted(phrases, key=lambda phrase: len(phrase))\n",
    "\n",
    "    return phrases_triees\n",
    "\n",
    "# Fonction pour diviser un fichier conllu en trois fichiers sans gérer de reste\n",
    "def diviser_fichier_conllu(fichier_conllu, fichier_sortie1, fichier_sortie2, fichier_sortie3):\n",
    "    with open(fichier_conllu, 'r', encoding='utf-8') as f:\n",
    "        # Lire toutes les phrases dans une liste\n",
    "        phrases_mix = list(parse_incr(f))\n",
    "\n",
    "    phrases = trier_phrases_par_taille(phrases_mix)\n",
    "    \n",
    "    # Calculer le nombre de phrases par groupe\n",
    "    total_phrases = len(phrases)\n",
    "    print(total_phrases)\n",
    "    taille_groupe = total_phrases // 3  # Arrondi à l'entier inférieur par division entière\n",
    "    print(taille_groupe)\n",
    "    # Répartir les phrases en trois groupes\n",
    "    groupe1 = phrases[:taille_groupe]\n",
    "    groupe2 = phrases[taille_groupe:2 * taille_groupe]\n",
    "    groupe3 = phrases[2 * taille_groupe:]\n",
    "    # groupe4 = phrases[3 * taille_groupe:]\n",
    "    print(2 * taille_groupe)\n",
    "    print(3 * taille_groupe)\n",
    "    \n",
    "    # Écrire chaque groupe dans son propre fichier\n",
    "    with open(fichier_sortie1, 'w', encoding='utf-8') as f1:\n",
    "        for phrase in groupe1:\n",
    "            f1.write(phrase.serialize())\n",
    "    \n",
    "    with open(fichier_sortie2, 'w', encoding='utf-8') as f2:\n",
    "        for phrase in groupe2:\n",
    "            f2.write(phrase.serialize())\n",
    "    \n",
    "    with open(fichier_sortie3, 'w', encoding='utf-8') as f3:\n",
    "        for phrase in groupe3:\n",
    "            f3.write(phrase.serialize())\n",
    "    \n",
    "    # with open(fichier_sortie4, 'w', encoding='utf-8') as f4:\n",
    "    #     for phrase in groupe4:\n",
    "    #         f4.write(phrase.serialize())\n",
    "    \n",
    "    print(f\"Le fichier a été divisé en trois fichiers : {fichier_sortie1}, {fichier_sortie2}, {fichier_sortie3}\")\n",
    "\n",
    "# Exemple d'utilisation\n",
    "fichier_conllu = \"/home/ziane212/Téléchargements/dump_MICLE_1563_Guernsey_30.08.24/last/expe_LLcD/train/1563_Guernsey_train.conllu\"\n",
    "fichier_sortie1 = \"/home/ziane212/Téléchargements/dump_MICLE_1563_Guernsey_30.08.24/last/expe_LLcD/train/size_split_train/1563_Guernsey_train_gr1.conllu\"\n",
    "fichier_sortie2 = \"/home/ziane212/Téléchargements/dump_MICLE_1563_Guernsey_30.08.24/last/expe_LLcD/train/size_split_train/1563_Guernsey_train_gr2.conllu\"\n",
    "fichier_sortie3 = \"/home/ziane212/Téléchargements/dump_MICLE_1563_Guernsey_30.08.24/last/expe_LLcD/train/size_split_train/1563_Guernsey_train_gr3.conllu\"\n",
    "# fichier_sortie4 = \"/home/ziane212/Téléchargements/dump_MICLE_1563_Guernsey_23.08.24/expe_LLcD/size_split/size_split_train/size_split_4_train_gr/1563_Guernsey_train_gr4.conllu\"\n",
    "\n",
    "diviser_fichier_conllu(fichier_conllu, fichier_sortie1, fichier_sortie2, fichier_sortie3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
