{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "connected to port: 42379\n"
     ]
    }
   ],
   "source": [
    "from grewpy import Corpus, Request\n",
    "\n",
    "path_in = \"/home/ziane212/Téléchargements/out_reparsing_extraction_1/1314_AttiremensetJugies_extraction04.07.24.conllu\"\n",
    "path_tsv = '/home/ziane212/Téléchargements/out_reparsing_extraction/1314_output.tsv'\n",
    "path_selected = '/home/ziane212/Téléchargements/out_reparsing_extraction/1314_selected_sents.tsv'\n",
    "\n",
    "corpus = Corpus(path_in)\n",
    "\n",
    "req_ccomp_noConj_subj = Request('X -[ccomp]-> Y; Y -[nsubj|expl]-> S').without('Y -[conj]-> Z; Y << Z; G [form = \"«\"]; X << G; G << Y; S [lemma=\"me\"|\"te\"|\"se\"]')\n",
    "res_ccomp_noConj_subj = corpus.search(req_ccomp_noConj_subj)\n",
    "\n",
    "req_ccomp_conj_subj = Request('R -[ccomp]-> X; X -[conj]-> Y; X << Y; Y -[nsubj|expl]-> S').without('G [form = \"«\"]; R << G; G << Y; S [lemma=\"me\"|\"te\"|\"se\"]')\n",
    "res_ccomp_conj_subj = corpus.search(req_ccomp_conj_subj)\n",
    "\n",
    "req_advcl_noConj_subj = Request('X -[advcl]-> Y; Y -[nsubj|expl]-> S').without('Y -[conj]-> Z; Y << Z; Y -[mark]-> M; M [upos=ADP]; S [lemma=\"me\"|\"te\"|\"se\"]')\n",
    "res_advcl_noConj_subj = corpus.search(req_advcl_noConj_subj)\n",
    "\n",
    "req_advcl_conj_subj = Request('R -[advcl]-> X; X -[conj]-> Y; X << Y; Y -[nsubj|expl]-> S').without('Y -[mark]-> M; M [upos=ADP]; S [lemma=\"me\"|\"te\"|\"se\"]')\n",
    "res_advcl_conj_subj = corpus.search(req_advcl_conj_subj)\n",
    "\n",
    "req_ccomp_noConj = Request('X -[ccomp]-> Y').without('Y -[conj]-> Z; Y << Z; G [form = \"«\"]; X << G; G << Y; Y -[nsubj|expl]-> S')\n",
    "res_ccomp_noConj = corpus.search(req_ccomp_noConj)\n",
    "\n",
    "req_ccomp_conj = Request('R -[ccomp]-> X; X -[conj]-> Y; X << Y').without('G [form = \"«\"]; R << G; G << Y; Y -[nsubj|expl]-> S')\n",
    "res_ccomp_conj = corpus.search(req_ccomp_conj)\n",
    "\n",
    "req_advcl_noConj = Request('X -[advcl]-> Y').without('Y -[conj]-> Z; Y << Z; Y -[mark]-> M; M [upos=ADP]; Y -[nsubj|expl]-> S')\n",
    "res_advcl_noConj = corpus.search(req_advcl_noConj)\n",
    "\n",
    "req_advcl_conj = Request('R -[advcl]-> X; X -[conj]-> Y; X << Y').without('Y -[mark]-> M; M [upos=ADP]; Y -[nsubj|expl]-> S')\n",
    "res_advcl_conj = corpus.search(req_advcl_conj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import conllu\n",
    "import csv\n",
    "\n",
    "\n",
    "def extract_sentences_with_results(path_in, results, clause_type, file_mode):\n",
    "    # Lire le fichier conllu\n",
    "    with open(path_in, 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    text = path_in.split('/')[-1]\n",
    "    \n",
    "    # Parse the conllu data\n",
    "    sentences = conllu.parse(data)\n",
    "    \n",
    "    # Créer un dictionnaire de phrases indexées par sent_id\n",
    "    sent_dict = {s.metadata['sent_id']: s for s in sentences}\n",
    "\n",
    "    # Ouvrir un fichier CSV pour écrire les résultats\n",
    "    with open(path_tsv, file_mode, newline='', encoding='utf-8') as tsvfile:\n",
    "        fieldnames = [\n",
    "            'Text', 'Sent_ID', 'Clause type', 'Verb position', 'Notes', \n",
    "            'Expressed subject', 'Preposed subject', 'Impersonal', \n",
    "            'Sujet nominal avec article', 'Preposed nominal objet', \n",
    "            'Particle', 'Left context', 'Pivot', 'Right Context'\n",
    "        ]\n",
    "        writer = csv.DictWriter(tsvfile, fieldnames=fieldnames, delimiter='\\t')\n",
    "        \n",
    "        if file_mode == 'w':\n",
    "            # Écrire l'en-tête seulement si le fichier est ouvert en mode écriture\n",
    "            writer.writeheader()\n",
    "        \n",
    "        # Parcourir les résultats\n",
    "        for result in results[1:]:\n",
    "            sent_id = result['sent_id']\n",
    "            if sent_id in sent_dict:\n",
    "                sentence = sent_dict[sent_id]\n",
    "                \n",
    "                y_id = int(result['matching']['nodes']['Y'])\n",
    "                pivot_id = ''\n",
    "                for token in sentence:\n",
    "                    if token['head']==y_id and (token['deprel']=='aux' or token['deprel']=='aux:pass' or token['deprel']=='cop'):\n",
    "                        pivot_id = token['id']\n",
    "                        break\n",
    "                # pivot_id = [token['id'] for token in sentence if token['head']==y_id and (token['deprel']=='aux' or token['deprel']=='aux:pass')][0]\n",
    "\n",
    "                if pivot_id == '':\n",
    "                    # Déterminer la position du verbe (Y)\n",
    "                    y_token = next((token for token in sentence if token['id'] == y_id), None)\n",
    "                    left_context = ' '.join([token['form'] for token in sentence[:y_id-1]])\n",
    "                    pivot = y_token['form']\n",
    "                    right_context = ' '.join([token['form'] for token in sentence[y_id:]])\n",
    "                else:\n",
    "                    y_token = next((token for token in sentence if token['id'] == pivot_id), None)\n",
    "                    left_context = ' '.join([token['form'] for token in sentence[:pivot_id-1]])\n",
    "                    pivot = y_token['form']\n",
    "                    right_context = ' '.join([token['form'] for token in sentence[pivot_id:]])\n",
    "\n",
    "                # Écrire la ligne dans le fichier CSV\n",
    "                if '_subj' in clause_type:\n",
    "                    writer.writerow({\n",
    "                        'Text': text,\n",
    "                        'Sent_ID': sent_id,\n",
    "                        'Clause type': clause_type,\n",
    "                        'Verb position': '',\n",
    "                        'Notes': '',\n",
    "                        'Expressed subject': 'Yes',\n",
    "                        'Preposed subject': '',\n",
    "                        'Impersonal': '',\n",
    "                        'Sujet nominal avec article': '',\n",
    "                        'Preposed nominal objet': '',\n",
    "                        'Particle': '',\n",
    "                        'Left context': left_context,\n",
    "                        'Pivot': pivot,\n",
    "                        'Right Context': right_context\n",
    "                    })\n",
    "                else:\n",
    "                    writer.writerow({\n",
    "                        'Text': text,\n",
    "                        'Sent_ID': sent_id,\n",
    "                        'Clause type': clause_type,\n",
    "                        'Verb position': '',\n",
    "                        'Notes': '',\n",
    "                        'Expressed subject': '',\n",
    "                        'Preposed subject': '',\n",
    "                        'Impersonal': '',\n",
    "                        'Sujet nominal avec article': '',\n",
    "                        'Preposed nominal objet': '',\n",
    "                        'Particle': '',\n",
    "                        'Left context': left_context,\n",
    "                        'Pivot': pivot,\n",
    "                        'Right Context': right_context\n",
    "                    })\n",
    "\n",
    "# Exécuter la fonction avec le chemin du fichier et les résultats\n",
    "extract_sentences_with_results(path_in, res_ccomp_noConj_subj, 'ccomp_noConj_subj', 'w')\n",
    "extract_sentences_with_results(path_in, res_ccomp_noConj, 'ccomp_noConj', 'a')\n",
    "extract_sentences_with_results(path_in, res_ccomp_conj_subj, 'ccomp_conj_subj', 'a')\n",
    "extract_sentences_with_results(path_in, res_ccomp_conj, 'ccomp_conj', 'a')\n",
    "extract_sentences_with_results(path_in, res_advcl_noConj_subj, 'advcl_noConj_subj', 'a')\n",
    "extract_sentences_with_results(path_in, res_advcl_noConj, 'advcl_noConj', 'a')\n",
    "extract_sentences_with_results(path_in, res_advcl_conj_subj, 'advcl_conj_subj', 'a')\n",
    "extract_sentences_with_results(path_in, res_advcl_conj, 'advcl_conj', 'a')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def remove_duplicate_lines(input_file, output_file):\n",
    "    df = pd.read_csv(input_file, sep='\\t')\n",
    "    df['Clause type'] = df['Clause type'].str.replace('_subj', '', regex=False)\n",
    "    \n",
    "    # Valeur spécifique à conserver dans la colonne\n",
    "    valeur_specifique = 'Yes'\n",
    "\n",
    "    # Séparation des lignes avec la valeur spécifique dans la colonne\n",
    "    df_specifique = df[df['Expressed subject'] == valeur_specifique]\n",
    "\n",
    "    # Séparation des autres lignes\n",
    "    df_autres = df[df['Expressed subject'] != valeur_specifique]\n",
    "\n",
    "    matching_columns = ['Text', 'Sent_ID', 'Clause type', 'Left context', 'Pivot', 'Right Context']\n",
    "\n",
    "    # Suppression des doublons dans les autres lignes en considérant uniquement les autres colonnes\n",
    "    df_autres_dedup = df_autres.drop_duplicates(subset=matching_columns)\n",
    "\n",
    "    # Combinaison des deux DataFrames\n",
    "    df_final = pd.concat([df_specifique, df_autres_dedup]).drop_duplicates(subset=matching_columns, keep='first')\n",
    "\n",
    "    # df = df.drop_duplicates()\n",
    "\n",
    "    df_final.to_csv(output_file, sep='\\t', index=False)\n",
    "\n",
    "remove_duplicate_lines(path_tsv, path_tsv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def select_phrases(input_file, output_file):\n",
    "    # Définir le nombre de phrases à sélectionner pour chaque type de clause\n",
    "    selection_counts = {\n",
    "        'ccomp_noConj': 245,\n",
    "        'ccomp_conj': 245,\n",
    "        'advcl_noConj': 225,\n",
    "        'advcl_conj': 175\n",
    "    }\n",
    "    \n",
    "    # Lire le fichier TSV\n",
    "    with open(input_file, 'r', encoding='utf-8') as tsvfile:\n",
    "        reader = csv.DictReader(tsvfile, delimiter='\\t')\n",
    "        rows = list(reader)\n",
    "    \n",
    "    # Séparer les phrases par type de clause\n",
    "    grouped_by_clause_type = {}\n",
    "    for row in rows:\n",
    "        clause_type = row['Clause type']\n",
    "        if clause_type not in grouped_by_clause_type:\n",
    "            grouped_by_clause_type[clause_type] = []\n",
    "        grouped_by_clause_type[clause_type].append(row)\n",
    "    \n",
    "    selected_phrases = []\n",
    "    \n",
    "    for clause_type, phrases in grouped_by_clause_type.items():\n",
    "        if clause_type not in selection_counts:\n",
    "            continue\n",
    "        \n",
    "        num_phrases = selection_counts[clause_type]\n",
    "        available_phrases = len(phrases)\n",
    "        \n",
    "        if available_phrases <= num_phrases:\n",
    "            # Si moins de phrases disponibles que demandé, sélectionner toutes les phrases\n",
    "            total_selected = phrases\n",
    "        else:\n",
    "            # Séparer les phrases courtes et longues\n",
    "            short_phrases = [p for p in phrases if len(p['Left context'].split() + [p['Pivot']] + p['Right Context'].split()) < 30]\n",
    "            long_phrases = [p for p in phrases if len(p['Left context'].split() + [p['Pivot']] + p['Right Context'].split()) >= 30]\n",
    "            \n",
    "            # Calculer le nombre de phrases courtes et longues nécessaires\n",
    "            num_short_phrases = int(num_phrases * 0.7)\n",
    "            num_long_phrases = num_phrases - num_short_phrases\n",
    "            \n",
    "            # Si moins de phrases courtes que nécessaire, ajuster\n",
    "            if len(short_phrases) < num_short_phrases:\n",
    "                num_short_phrases = len(short_phrases)\n",
    "                num_long_phrases = num_phrases - num_short_phrases\n",
    "            # Si moins de phrases longues que nécessaire, ajuster\n",
    "            if len(long_phrases) < num_long_phrases:\n",
    "                num_long_phrases = len(long_phrases)\n",
    "                num_short_phrases = num_phrases - num_long_phrases            \n",
    "            \n",
    "            # Prendre les phrases courtes\n",
    "            selected_short_phrases = random.sample(short_phrases, num_short_phrases)\n",
    "            \n",
    "            # Prendre les phrases longues\n",
    "            selected_long_phrases = random.sample(long_phrases, num_long_phrases)\n",
    "            \n",
    "            total_selected = selected_short_phrases + selected_long_phrases\n",
    "        \n",
    "        for phrase in total_selected:\n",
    "            phrase['Length'] = 'short' if len(phrase['Left context'].split() + [phrase['Pivot']] + phrase['Right Context'].split()) < 30 else 'long'\n",
    "        \n",
    "        selected_phrases.extend(total_selected)\n",
    "    \n",
    "    # Écrire les phrases sélectionnées dans un nouveau fichier TSV\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as tsvfile:\n",
    "        fieldnames = [\n",
    "            'Text', 'Sent_ID', 'Clause type', 'Verb position', 'Notes', \n",
    "            'Expressed subject', 'Preposed subject', 'Impersonal', \n",
    "            'Sujet nominal avec article', 'Preposed nominal objet', \n",
    "            'Particle', 'Left context', 'Pivot', 'Right Context', 'Length'\n",
    "        ]\n",
    "        writer = csv.DictWriter(tsvfile, fieldnames=fieldnames, delimiter='\\t')\n",
    "        writer.writeheader()\n",
    "        \n",
    "        for row in selected_phrases:\n",
    "            writer.writerow(row)\n",
    "\n",
    "select_phrases(path_tsv, \n",
    "               path_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
